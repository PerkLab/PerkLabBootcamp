{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "from skimage.util import montage\n",
    "# import medmnist\n",
    "# from medmnist import INFO, Evaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "data_file = 'breastmnist.npz'\n",
    "npz_data = np.load(data_file)\n",
    "print(npz_data.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = npz_data['train_images']\n",
    "train_labels = npz_data['train_labels']\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "print(np.unique(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.random.randint(len(train_images))\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(train_images[ind], cmap='gray')\n",
    "plt.title('label:'+str(train_labels[ind]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.imshow(montage(train_images[:49]), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BreastDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, npz_data, split, transform = None):\n",
    "        self.npz_data = npz_data\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        \n",
    "        if self.split == 'train':\n",
    "            self.imgs = npz_data['train_images']\n",
    "            self.labels = npz_data['train_labels']\n",
    "        elif self.split == 'val':\n",
    "            self.imgs = npz_data['val_images']\n",
    "            self.labels = npz_data['val_labels']\n",
    "        elif self.split == 'test':\n",
    "            self.imgs = npz_data['test_images']\n",
    "            self.labels = npz_data['test_labels']\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.imgs.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        img, target = self.imgs[index], self.labels[index].astype(int)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        return img, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "lr = 0.001\n",
    "n_channels = 1\n",
    "n_classes = 2\n",
    "\n",
    "seed = 20\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "train_dataset = BreastDataset(npz_data, 'train', data_transform)\n",
    "val_dataset = BreastDataset(npz_data, 'val', data_transform)\n",
    "test_dataset = BreastDataset(npz_data, 'test', data_transform)\n",
    "\n",
    "train_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = data.DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, kernel_size=3),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(32, 32, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(32 * 4 * 4, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "model = Net(in_channels=n_channels, num_classes=n_classes)\n",
    "\n",
    "# define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# metric (accuracy)\n",
    "def accuracy(scores, targets):\n",
    "    scores = scores.detach().argmax(dim=1)\n",
    "    acc = (scores==targets).float().sum().item()\n",
    "    acc = acc / len(targets) # i'm not sure if needed or not\n",
    "    return acc\n",
    "\n",
    "# epoch train function\n",
    "def train_epoch(model, optimizer, data_loader, criterion, num_classes):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch_ind, (batch_inputs, batch_labels) in enumerate(tqdm(data_loader)):\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward\n",
    "        batch_scores = model.forward(batch_inputs)\n",
    "        batch_labels = batch_labels.squeeze().long()\n",
    "        loss = criterion(batch_scores, batch_labels)   \n",
    "        \n",
    "        # backward + optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.detach().item()\n",
    "        epoch_acc += accuracy(batch_scores, batch_labels)\n",
    "        \n",
    "        prob = batch_scores.softmax(dim=-1)\n",
    "        \n",
    "    epoch_loss /= (batch_ind + 1)\n",
    "    epoch_acc /= (batch_ind + 1)\n",
    "    \n",
    "    return epoch_loss, epoch_acc, optimizer\n",
    "\n",
    "\n",
    "# epoch evaluate function\n",
    "def evaluate_epoch(model, data_loader, criterion, num_classes):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # since not training, no need to calculate the gradient\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch_ind, (batch_inputs, batch_labels) in enumerate(data_loader):\n",
    "            \n",
    "            batch_scores = model.forward(batch_inputs)\n",
    "            batch_labels = batch_labels.squeeze().long()\n",
    "            loss = criterion(batch_scores, batch_labels)   \n",
    "\n",
    "            epoch_loss += loss.detach().item()\n",
    "            epoch_acc += accuracy(batch_scores, batch_labels)\n",
    "            \n",
    "            prob = batch_scores.softmax(dim=-1)\n",
    "\n",
    "        epoch_loss /= (batch_ind + 1)\n",
    "        epoch_acc /= (batch_ind + 1)\n",
    "    \n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training loop\n",
    "train_loss, train_acc = [], []\n",
    "val_loss, val_acc = [], []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print('epoch', epoch)\n",
    "\n",
    "    train_epoch_loss, train_epoch_acc, optimizer = train_epoch(model, optimizer, train_loader, criterion, n_classes)\n",
    "    print('train loss/acc: {:.2f}, {:.2f}'.format(train_epoch_loss, train_epoch_acc))\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    train_acc.append(train_epoch_acc)\n",
    "    \n",
    "    val_epoch_loss, val_epoch_acc = evaluate_epoch(model, val_loader, criterion, n_classes)\n",
    "    print('val loss/acc: {:.2f}, {:.2f}'.format(val_epoch_loss, val_epoch_acc))\n",
    "    val_loss.append(val_epoch_loss)\n",
    "    val_acc.append(val_epoch_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visulization of learning curves\n",
    "plt.plot(train_loss)\n",
    "plt.plot(val_loss)\n",
    "plt.legend(['train','val'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_acc)\n",
    "plt.plot(val_acc)\n",
    "plt.legend(['train','val'])\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n",
    "\n",
    "# evaluation on test set\n",
    "epoch_loss, epoch_acc = evaluate_epoch(model, test_loader, criterion, n_classes)\n",
    "print('test loss/acc: {:.2f}, {:.2f}'.format(epoch_loss, epoch_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "model = Net(in_channels=n_channels, num_classes=n_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStop:\n",
    "    def __init__(self, patience=1):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.best_model = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss >= self.val_loss_min:\n",
    "            self.counter += 1\n",
    "            print(f'ES counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.val_loss_min = val_loss\n",
    "            self.best_model = copy.deepcopy(model)\n",
    "            self.counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loss, train_acc = [], []\n",
    "val_loss, val_acc = [], []\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "ES_PATIENCE = 5\n",
    "early_stopping = EarlyStop(patience=ES_PATIENCE)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print('epoch', epoch)\n",
    "\n",
    "    train_epoch_loss, train_epoch_acc, optimizer = train_epoch(model, optimizer, train_loader, criterion, n_classes)\n",
    "    print('train loss/acc: {:.2f}, {:.2f}'.format(train_epoch_loss, train_epoch_acc))\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    train_acc.append(train_epoch_acc)\n",
    "    \n",
    "    val_epoch_loss, val_epoch_acc = evaluate_epoch(model, val_loader, criterion, n_classes)\n",
    "    print('val loss/acc: {:.2f}, {:.2f}'.format(val_epoch_loss, val_epoch_acc))\n",
    "    val_loss.append(val_epoch_loss)\n",
    "    val_acc.append(val_epoch_acc)\n",
    "    \n",
    "    early_stopping(val_epoch_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"early stopping at epoch {}, model saved at epoch {}\".format(epoch, epoch-early_stopping.patience))\n",
    "        break\n",
    "    \n",
    "    if epoch==0:\n",
    "        init_model = copy.deepcopy(model)\n",
    "\n",
    "best_model = early_stopping.best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss)\n",
    "plt.plot(val_loss)\n",
    "plt.legend(['train','val'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_acc)\n",
    "plt.plot(val_acc)\n",
    "plt.legend(['train','val'])\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_loss, epoch_acc = evaluate_epoch(init_model, test_loader, criterion, n_classes)\n",
    "print('1st epoch test loss/acc: {:.2f}, {:.2f}'.format(epoch_loss, epoch_acc))\n",
    "\n",
    "epoch_loss, epoch_acc = evaluate_epoch(model, test_loader, criterion, n_classes)\n",
    "print('last epoch test loss/acc: {:.2f}, {:.2f}'.format(epoch_loss, epoch_acc))\n",
    "\n",
    "epoch_loss, epoch_acc = evaluate_epoch(best_model, test_loader, criterion, n_classes)\n",
    "print('best model test loss/acc: {:.2f}, {:.2f}'.format(epoch_loss, epoch_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_single = data.DataLoader(dataset=test_dataset, batch_size=1,shuffle=False)\n",
    "\n",
    "probs = []\n",
    "labels = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for image, label in test_single:\n",
    "        score= model.forward(image)\n",
    "        prob= score.softmax(dim=-1)\n",
    "        \n",
    "        labels.append(label.detach().item())\n",
    "        probs.append(prob.detach().numpy())\n",
    "\n",
    "probs = np.concatenate(probs,axis=0)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "cm = confusion_matrix(labels, np.argmax(probs, axis=1), normalize='true')\n",
    "ConfusionMatrixDisplay(cm).plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
